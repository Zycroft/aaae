---
phase: 28-testing-verification
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/src/provider/providerFactory.test.ts
  - server/src/config.test.ts
autonomous: true
requirements: [TEST-03, TEST-05]

must_haves:
  truths:
    - "Provider factory creates OpenAiProvider when LLM_PROVIDER=openai"
    - "Provider factory creates CopilotProvider when LLM_PROVIDER=copilot"
    - "Provider factory uses dynamic import for lazy-loading"
    - "getProviderInfo returns correct provider name and model"
    - "Config exits with fatal error for invalid LLM_PROVIDER value"
    - "Config exits with fatal error when OPENAI_API_KEY missing for openai provider"
    - "Config exits with fatal error when COPILOT_ENVIRONMENT_ID missing for copilot provider"
  artifacts:
    - path: "server/src/provider/providerFactory.test.ts"
      provides: "Provider factory unit tests"
      min_lines: 60
    - path: "server/src/config.test.ts"
      provides: "Config validation unit tests"
      min_lines: 40
  key_links:
    - from: "server/src/provider/providerFactory.test.ts"
      to: "server/src/provider/providerFactory.ts"
      via: "import createProvider, getProviderInfo"
      pattern: "import.*providerFactory"
    - from: "server/src/config.test.ts"
      to: "server/src/config.ts"
      via: "subprocess or dynamic import with env override"
      pattern: "LLM_PROVIDER|OPENAI_API_KEY|COPILOT_ENVIRONMENT_ID"
---

<objective>
Create unit tests for the provider factory and config validation modules, satisfying TEST-03 and TEST-05.

Purpose: The provider factory (providerFactory.ts) and config validation (config.ts) are untested. TEST-03 requires tests confirming the factory creates the correct provider per config. TEST-05 requires tests asserting fatal errors for each bad-config scenario.

Output: Two new test files — providerFactory.test.ts and config.test.ts — with all scenarios covered.
</objective>

<execution_context>
@/Users/zycroft/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zycroft/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@server/src/provider/providerFactory.ts
@server/src/provider/LlmProvider.ts
@server/src/config.ts
@server/src/provider/OpenAiProvider.ts
@server/src/provider/CopilotProvider.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Provider factory unit tests (TEST-03)</name>
  <files>server/src/provider/providerFactory.test.ts</files>
  <action>
Create server/src/provider/providerFactory.test.ts with Vitest. Test both `createProvider()` and `getProviderInfo()`.

**getProviderInfo() tests:**
1. Returns `{ provider: 'openai', model: 'gpt-4o-mini' }` when config.LLM_PROVIDER is 'openai' and config.OPENAI_MODEL is 'gpt-4o-mini'
2. Returns `{ provider: 'copilot', model: 'copilot-studio' }` when config.LLM_PROVIDER is 'copilot'
3. Returns custom model value when OPENAI_MODEL is set to 'gpt-4o'

**createProvider() tests:**
Mock the config module (`vi.mock('../config.js')`) and mock the dynamic imports for OpenAiProvider and CopilotProvider modules. Use `vi.mock()` for the provider modules themselves:
- Mock `./OpenAiProvider.js` to export a class with a constructor spy
- Mock `./CopilotProvider.js` to export a class with a constructor spy
- Mock `../copilot.js` to export a `copilotClient` object

Test cases:
1. When `config.LLM_PROVIDER === 'openai'`: `createProvider()` creates an OpenAiProvider instance with `{ apiKey, model }` from config
2. When `config.LLM_PROVIDER === 'copilot'`: `createProvider()` creates a CopilotProvider instance with copilotClient
3. Console.log is called with provider name for both paths

Use `vi.mocked(config)` to control config values per test. Each test should set the appropriate config values in `beforeEach` or inline.

**Important patterns:**
- Import `config` after vi.mock to get the mocked version
- Use `vi.fn()` constructors that return mock LlmProvider-shaped objects
- The createProvider function is async (uses dynamic import) — await it in tests
  </action>
  <verify>cd server && npx vitest run src/provider/providerFactory.test.ts</verify>
  <done>providerFactory.test.ts passes with tests covering both createProvider paths and all getProviderInfo scenarios</done>
</task>

<task type="auto">
  <name>Task 2: Config validation tests (TEST-05)</name>
  <files>server/src/config.test.ts</files>
  <action>
Create server/src/config.test.ts with Vitest. Config validation happens at module-load time via `process.exit(1)` calls, so tests must verify exit behavior.

**Strategy:** Since config.ts runs validation at module scope (top-level code), use `vi.spyOn(process, 'exit').mockImplementation()` and `vi.spyOn(console, 'error').mockImplementation()` before dynamically importing config.ts with controlled environment variables.

**Approach — use a subprocess per test case:**
For each bad-config scenario, spawn a child process with controlled env vars and assert it exits with code 1 and the correct FATAL message in stderr. Use Node.js `child_process.execSync` or `execFileSync` with a tiny script that just `import('./config.js')`.

Alternative simpler approach: Reset modules between tests using `vi.resetModules()` and control `process.env` directly:
```typescript
beforeEach(() => {
  vi.resetModules();
  vi.spyOn(process, 'exit').mockImplementation((() => { throw new Error('process.exit'); }) as never);
  vi.spyOn(console, 'error').mockImplementation(() => {});
  // Clear all relevant env vars
  delete process.env.LLM_PROVIDER;
  delete process.env.OPENAI_API_KEY;
  delete process.env.COPILOT_ENVIRONMENT_ID;
  delete process.env.COPILOT_AGENT_SCHEMA_NAME;
  delete process.env.AUTH_REQUIRED;
  delete process.env.AZURE_CLIENT_ID;
});
```

**Test cases:**
1. **Invalid LLM_PROVIDER value** — Set `LLM_PROVIDER='invalid_value'`, `AUTH_REQUIRED='false'`. Dynamic import config.ts. Assert `process.exit` was called and `console.error` was called with message containing `FATAL` and `not valid`.
2. **Missing OPENAI_API_KEY for openai provider** — Set `LLM_PROVIDER='openai'`, no `OPENAI_API_KEY`, `AUTH_REQUIRED='false'`. Assert fatal exit with message about `OPENAI_API_KEY`.
3. **Missing COPILOT_ENVIRONMENT_ID for copilot** — Set `LLM_PROVIDER='copilot'`, no `COPILOT_ENVIRONMENT_ID`, `AUTH_REQUIRED='false'`. Assert fatal exit mentioning `COPILOT_ENVIRONMENT_ID`.
4. **Missing COPILOT_AGENT_SCHEMA_NAME for copilot** — Set `LLM_PROVIDER='copilot'`, set `COPILOT_ENVIRONMENT_ID='test'`, no `COPILOT_AGENT_SCHEMA_NAME`, `AUTH_REQUIRED='false'`. Assert fatal exit mentioning `COPILOT_AGENT_SCHEMA_NAME`.
5. **Missing AZURE_CLIENT_ID when AUTH_REQUIRED is true** — Set `LLM_PROVIDER='openai'`, `OPENAI_API_KEY='test-key'`, no `AUTH_REQUIRED` (defaults to true), no `AZURE_CLIENT_ID`. Assert fatal exit mentioning `AZURE_CLIENT_ID`.
6. **Valid openai config does not exit** — Set `LLM_PROVIDER='openai'`, `OPENAI_API_KEY='test-key'`, `AUTH_REQUIRED='false'`. Import should succeed without process.exit.
7. **Valid copilot config does not exit** — Set `LLM_PROVIDER='copilot'`, `COPILOT_ENVIRONMENT_ID='test'`, `COPILOT_AGENT_SCHEMA_NAME='test'`, `AUTH_REQUIRED='false'`. Import should succeed.

Note: config.ts imports 'dotenv/config' at the top. Mock it: `vi.mock('dotenv/config', () => ({}))` to prevent .env file interference.
  </action>
  <verify>cd server && npx vitest run src/config.test.ts</verify>
  <done>config.test.ts passes with tests covering all bad-config fatal scenarios (invalid provider, missing API key, missing Copilot vars, missing Azure client ID) and valid config scenarios</done>
</task>

</tasks>

<verification>
1. `cd server && npx vitest run src/provider/providerFactory.test.ts` — all provider factory tests pass
2. `cd server && npx vitest run src/config.test.ts` — all config validation tests pass
3. `cd server && npx vitest run` — all existing + new tests pass together (no regressions)
</verification>

<success_criteria>
- Provider factory tests confirm correct provider creation per config (TEST-03)
- Config validation tests assert correct fatal error for each bad-config scenario (TEST-05)
- All tests pass without regressions
- No test isolation issues (each test cleans up env vars and module state)
</success_criteria>

<output>
After completion, create `.planning/phases/28-testing-verification/28-01-SUMMARY.md`
</output>
