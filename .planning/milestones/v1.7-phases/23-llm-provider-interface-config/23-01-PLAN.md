---
phase: 23-llm-provider-interface-config
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/src/provider/LlmProvider.ts
  - server/src/config.ts
  - server/.env.example
autonomous: true
requirements:
  - PROV-01
  - CONF-01
  - CONF-02
  - CONF-03

must_haves:
  truths:
    - "Server starts cleanly with LLM_PROVIDER=copilot and existing Copilot env vars — no error, no new warnings"
    - "Server starts cleanly with LLM_PROVIDER=openai and OPENAI_API_KEY — without any Copilot env vars present"
    - "Server exits with a fatal error when LLM_PROVIDER=openai but OPENAI_API_KEY is missing"
    - "Server exits with a fatal error when LLM_PROVIDER=copilot but Copilot env vars are missing"
    - "LlmProvider interface file exists with startSession, sendMessage, sendCardAction method signatures"
  artifacts:
    - path: "server/src/provider/LlmProvider.ts"
      provides: "LlmProvider interface with startSession, sendMessage, sendCardAction"
      exports: ["LlmProvider"]
    - path: "server/src/config.ts"
      provides: "Conditional provider-aware env var validation"
      contains: "LLM_PROVIDER"
    - path: "server/.env.example"
      provides: "Documentation for new env vars LLM_PROVIDER and OPENAI_API_KEY"
      contains: "LLM_PROVIDER"
  key_links:
    - from: "server/src/config.ts"
      to: "LLM_PROVIDER env var"
      via: "conditional branch replacing unconditional REQUIRED array"
      pattern: "LLM_PROVIDER"
    - from: "server/src/provider/LlmProvider.ts"
      to: "NormalizedMessage"
      via: "import from @copilot-chat/shared"
      pattern: "NormalizedMessage"
---

<objective>
Define the LlmProvider interface contract and update server config validation to be provider-conditional — Copilot vars only required for Copilot, OpenAI vars only required for OpenAI.

Purpose: Establishes the interface that CopilotProvider (Phase 24) and OpenAiProvider (Phase 26) will implement, and ensures the server fails loudly with the correct error for any bad config scenario.
Output: server/src/provider/LlmProvider.ts (interface), updated server/src/config.ts (conditional validation), updated server/.env.example (new vars documented).
</objective>

<execution_context>
@/Users/zycroft/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zycroft/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@server/src/config.ts
@server/src/orchestrator/WorkflowOrchestrator.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LlmProvider interface</name>
  <files>server/src/provider/LlmProvider.ts</files>
  <action>
Create the directory `server/src/provider/` and the file `server/src/provider/LlmProvider.ts`.

The interface must define three async methods, all returning `Promise<NormalizedMessage[]>`:

```typescript
import type { NormalizedMessage } from '@copilot-chat/shared';

/**
 * Contract for all LLM backends. Implementations handle normalization
 * internally — callers receive NormalizedMessage[] only.
 *
 * startSession  — initialise a new conversation; may return a greeting message or empty array
 * sendMessage   — send a user text turn and return the assistant's response messages
 * sendCardAction — forward an Adaptive Card submit action and return response messages
 */
export interface LlmProvider {
  startSession(conversationId: string): Promise<NormalizedMessage[]>;
  sendMessage(conversationId: string, message: string): Promise<NormalizedMessage[]>;
  sendCardAction(
    conversationId: string,
    actionValue: Record<string, unknown>
  ): Promise<NormalizedMessage[]>;
}
```

No implementation code — interface only. The file must compile cleanly with `cd server && npx tsc --noEmit`.
  </action>
  <verify>
    <automated>cd /Users/zycroft/Documents/PA/aaae/server && npx tsc --noEmit 2>&1 | head -20</automated>
    <manual>Confirm server/src/provider/LlmProvider.ts exists and exports LlmProvider interface with all three method signatures.</manual>
  </verify>
  <done>File exists, exports LlmProvider interface with startSession/sendMessage/sendCardAction, TypeScript compiles cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Update config.ts with conditional provider validation + update .env.example</name>
  <files>server/src/config.ts, server/.env.example</files>
  <action>
Replace the existing `server/src/config.ts` unconditional `REQUIRED` check with provider-conditional validation logic. The new logic must:

1. Read `LLM_PROVIDER` from env, defaulting to `'copilot'` if not set. Validate it is either `'copilot'` or `'openai'`; if not, log a fatal error and exit.
2. If `LLM_PROVIDER === 'copilot'`: require `COPILOT_ENVIRONMENT_ID` and `COPILOT_AGENT_SCHEMA_NAME` (same as current behaviour). Log `[config] FATAL: Missing required env var: X` and `process.exit(1)` for each missing var.
3. If `LLM_PROVIDER === 'openai'`: require `OPENAI_API_KEY`. Log `[config] FATAL: LLM_PROVIDER=openai but OPENAI_API_KEY is not set.` and `process.exit(1)` if missing. Do NOT require any `COPILOT_*` vars.
4. Keep the existing Azure AD conditional check (`AUTH_REQUIRED !== 'false'` requires `AZURE_CLIENT_ID`) unchanged.
5. Add to the exported `config` object:
   - `LLM_PROVIDER: process.env.LLM_PROVIDER as 'copilot' | 'openai'` (already validated above)
   - `OPENAI_API_KEY: process.env.OPENAI_API_KEY` (optional string, only present when provider is openai)
   - `OPENAI_MODEL: process.env.OPENAI_MODEL ?? 'gpt-4o-mini'`
   Keep all existing config fields unchanged (COPILOT_*, AUTH_REQUIRED, CORS_ORIGIN, PORT, AZURE_*, ALLOWED_TENANT_IDS, REDIS_*).

The config object's `COPILOT_ENVIRONMENT_ID` and `COPILOT_AGENT_SCHEMA_NAME` fields are no longer `!`-asserted unconditionally — use `process.env.COPILOT_ENVIRONMENT_ID ?? ''` since validation is now conditional. All other `COPILOT_*` optional fields remain as optional (`process.env.COPILOT_*`).

For `server/.env.example`: add a new section `# --- LLM Provider (Phase 23+) ---` before the Copilot section. Document:
- `LLM_PROVIDER` — values: `copilot` (default) or `openai`
- `OPENAI_API_KEY` — required only when `LLM_PROVIDER=openai`
- `OPENAI_MODEL` — optional, defaults to `gpt-4o-mini`

Keep all existing .env.example content unchanged below the new section.
  </action>
  <verify>
    <automated>cd /Users/zycroft/Documents/PA/aaae/server && npx tsc --noEmit 2>&1 | head -20</automated>
    <manual>
    Confirm config validation by running these spot-checks (use env -i to start with empty environment):

    1. LLM_PROVIDER=openai + OPENAI_API_KEY set — should print nothing fatal and exit 0 (or hang waiting for port, meaning it started):
       cd server && LLM_PROVIDER=openai OPENAI_API_KEY=sk-test AUTH_REQUIRED=false node -e "require('./dist/config.js')" 2>&1 | head -5

    2. LLM_PROVIDER=openai without OPENAI_API_KEY — should print FATAL and exit 1:
       cd server && LLM_PROVIDER=openai AUTH_REQUIRED=false node -e "require('./dist/config.js')" 2>&1 | grep FATAL

    3. LLM_PROVIDER=copilot without Copilot vars — should print FATAL and exit 1:
       cd server && LLM_PROVIDER=copilot AUTH_REQUIRED=false node -e "require('./dist/config.js')" 2>&1 | grep FATAL

    Since config.ts uses TypeScript source (tsx watch), verify by running tsc --noEmit. Smoke tests against compiled output are optional manual checks.
    </manual>
  </verify>
  <done>
    config.ts: LLM_PROVIDER added to exports, Copilot vars required only when LLM_PROVIDER=copilot, OPENAI_API_KEY required only when LLM_PROVIDER=openai, TypeScript compiles cleanly.
    .env.example: LLM_PROVIDER, OPENAI_API_KEY, OPENAI_MODEL documented in new section.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `cd /Users/zycroft/Documents/PA/aaae/server && npx tsc --noEmit` — zero errors
2. `npm test` from repo root passes (existing tests unbroken — config.ts is imported by many modules, the change must remain backward-compatible for the Copilot default path)
3. `server/src/provider/LlmProvider.ts` exists and `grep -n 'startSession\|sendMessage\|sendCardAction' server/src/provider/LlmProvider.ts` shows all three signatures
4. `grep 'LLM_PROVIDER' server/src/config.ts` shows the new field and validation logic
5. `grep 'LLM_PROVIDER' server/.env.example` shows documentation
</verification>

<success_criteria>
- LlmProvider interface file exists at server/src/provider/LlmProvider.ts with startSession, sendMessage, sendCardAction
- server/src/config.ts validates COPILOT_* vars only when LLM_PROVIDER=copilot
- server/src/config.ts validates OPENAI_API_KEY only when LLM_PROVIDER=openai
- server/src/config.ts exports LLM_PROVIDER, OPENAI_API_KEY, OPENAI_MODEL
- TypeScript compiles without errors
- All pre-existing server tests pass (npm test from root)
- server/.env.example documents the three new env vars
</success_criteria>

<output>
After completion, create `.planning/phases/23-llm-provider-interface-config/23-01-SUMMARY.md`
</output>
