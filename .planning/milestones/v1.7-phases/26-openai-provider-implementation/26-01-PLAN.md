---
phase: 26-openai-provider-implementation
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - server/package.json
  - server/src/provider/OpenAiProvider.ts
  - server/src/provider/OpenAiProvider.test.ts
autonomous: true
requirements: [OAPI-01, OAPI-02, OAPI-03, OAPI-04, OAPI-05, OAPI-06]

must_haves:
  truths:
    - "OpenAiProvider implements LlmProvider and compiles without error"
    - "startSession returns a greeting NormalizedMessage[] with extractedPayload containing action:'ask'"
    - "sendMessage includes accumulated conversation history in the OpenAI request"
    - "sendMessage returns NormalizedMessage[] with extractedPayload from structured output"
    - "sendCardAction converts action payload to text description and delegates to sendMessage internally"
    - "OpenAI model is read from OPENAI_MODEL env var, defaulting to gpt-4o-mini"
  artifacts:
    - path: "server/src/provider/OpenAiProvider.ts"
      provides: "OpenAI-backed LlmProvider implementation"
      exports: ["OpenAiProvider"]
    - path: "server/src/provider/OpenAiProvider.test.ts"
      provides: "Unit tests with mocked OpenAI SDK"
      contains: "vi.mock"
  key_links:
    - from: "server/src/provider/OpenAiProvider.ts"
      to: "server/src/provider/LlmProvider.ts"
      via: "implements LlmProvider"
      pattern: "implements LlmProvider"
    - from: "server/src/provider/OpenAiProvider.ts"
      to: "openai"
      via: "OpenAI chat.completions.create"
      pattern: "chat\\.completions\\.create"
    - from: "server/src/provider/OpenAiProvider.ts"
      to: "@copilot-chat/shared"
      via: "NormalizedMessage construction with extractedPayload"
      pattern: "extractedPayload"
---

<objective>
Create `OpenAiProvider` implementing `LlmProvider` using the OpenAI chat completions API with TDD.

Purpose: This is the core new functionality for v1.7 — an alternative LLM backend that enables running the app without Copilot Studio credentials. The provider manages per-conversation history, uses structured output for `extractedPayload` compatibility, and converts card actions to text.

Output: Working, tested `OpenAiProvider` class with full unit test coverage.
</objective>

<execution_context>
@/Users/zycroft/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zycroft/.claude/get-shit-done/templates/summary.md
@/Users/zycroft/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@server/src/provider/LlmProvider.ts
@server/src/provider/CopilotProvider.ts
@server/src/provider/CopilotProvider.test.ts
@server/src/config.ts
@shared/src/schemas/message.ts
@shared/src/schemas/extractedPayload.ts
@shared/src/schemas/workflow.ts
@server/src/normalizer/activityNormalizer.ts
@server/src/parser/structuredOutputParser.ts
@server/src/workflow/contextBuilder.ts
@server/package.json
</context>

<feature>
  <name>OpenAiProvider — OpenAI chat completions LlmProvider</name>
  <files>server/src/provider/OpenAiProvider.ts, server/src/provider/OpenAiProvider.test.ts</files>
  <behavior>
    **startSession(conversationId):**
    - Initializes empty conversation history for this conversationId
    - Sends a system prompt to OpenAI with `response_format: { type: "json_schema", json_schema: ... }` requesting a greeting
    - The JSON schema must produce objects matching `CopilotStructuredOutputSchema`: `{ action, prompt, data, confidence, citations }`
    - Returns `NormalizedMessage[]` with a single text message containing the greeting text
    - The message's `extractedPayload` field is set with `source: 'value'`, `confidence: 'high'`, and `data` containing the structured output
    - The assistant's response is appended to conversation history

    **sendMessage(conversationId, message):**
    - Appends user message to conversation history as `{ role: 'user', content: message }`
    - Calls OpenAI `chat.completions.create` with:
      - `model`: from config (`OPENAI_MODEL` env var, default `gpt-4o-mini`)
      - `messages`: system prompt + full conversation history (OAPI-03, OAPI-04)
      - `response_format`: JSON schema matching `CopilotStructuredOutputSchema` (OAPI-02)
    - The system prompt template injects workflow context: step, collectedData, turnCount (OAPI-03)
      - Note: The orchestrator's `contextBuilder` already enriches the user message text with `[CONTEXT]...[/CONTEXT]` preamble. The OpenAI system prompt should instruct the model about the expected structured output format, the workflow steps (initial, gather_info, research, confirm, complete), and how to produce action/prompt/data/confidence fields.
    - Parses the JSON response into `CopilotStructuredOutputSchema` shape
    - Appends assistant response to conversation history
    - Constructs `NormalizedMessage` with:
      - `id`: UUID
      - `role: 'assistant'`
      - `kind: 'text'`
      - `text`: the `prompt` field from structured output (the user-facing text)
      - `extractedPayload: { source: 'value', confidence: 'high', data: <parsed structured output> }`
    - Returns `NormalizedMessage[]` (single-element array)

    **sendCardAction(conversationId, actionValue):**
    - Converts `actionValue: Record<string, unknown>` to a human-readable text description
    - Format: `"[Card Action] User submitted: {key1}: {value1}, {key2}: {value2}"` (or similar)
    - Delegates to `this.sendMessage(conversationId, textDescription)` (OAPI-05)
    - Returns the result from sendMessage

    **History management:**
    - `Map<string, Array<{ role: 'system' | 'user' | 'assistant'; content: string }>>` keyed by conversationId
    - System prompt is prepended to every call (not stored in history array)
    - History grows across turns for multi-turn context (OAPI-04)

    **Structured output JSON schema:**
    The `response_format` parameter uses OpenAI's `json_schema` type:
    ```
    response_format: {
      type: "json_schema",
      json_schema: {
        name: "workflow_response",
        strict: true,
        schema: {
          type: "object",
          properties: {
            action: { type: "string", enum: ["ask", "research", "confirm", "complete", "error"] },
            prompt: { type: "string" },
            data: { type: "object", additionalProperties: true },
            confidence: { type: "number" },
            citations: { type: "array", items: { type: "string" } }
          },
          required: ["action", "prompt", "data", "confidence", "citations"],
          additionalProperties: false
        }
      }
    }
    ```
    This ensures every response is parseable and matches `CopilotStructuredOutputSchema`.

    **Test cases (RED phase — all must fail initially):**
    1. `startSession` returns NormalizedMessage[] with greeting text and extractedPayload
    2. `sendMessage` calls OpenAI with system prompt + accumulated history
    3. `sendMessage` on second call includes first turn in history (OAPI-04)
    4. `sendMessage` returns NormalizedMessage with extractedPayload containing structured data
    5. `sendCardAction` converts action to text and delegates to sendMessage (OAPI-05)
    6. Constructor uses `OPENAI_MODEL` from config, defaults to `gpt-4o-mini` (OAPI-06)
    7. `extractedPayload.data` passes `CopilotStructuredOutputSchema.safeParse()` validation
  </behavior>
  <implementation>
    **Dependencies:**
    - Install `openai` npm package in server workspace: `npm install openai --workspace=server`

    **OpenAiProvider class:**
    - Constructor takes `{ apiKey: string; model?: string }` — reads from `config.OPENAI_API_KEY` and `config.OPENAI_MODEL`
    - Creates `OpenAI` client instance with the API key
    - Stores model name (default: `gpt-4o-mini`)
    - Initializes `conversationHistories: Map<string, ChatMessage[]>`

    **System prompt design:**
    The system prompt instructs the OpenAI model to:
    1. Act as a helpful assistant for a workflow-driven application
    2. Always respond in the exact JSON schema format
    3. Use the `action` field to signal workflow state transitions
    4. Use the `prompt` field for user-facing text
    5. Use the `data` field to collect/return structured information
    6. Set `confidence` between 0-1 based on certainty
    7. Include `citations` as an empty array unless referencing sources
    8. Understand the workflow steps: initial -> gather_info -> research -> confirm -> complete

    **Mock strategy for tests:**
    - Mock `openai` module using `vi.mock('openai', ...)`
    - Mock `chat.completions.create` to return predictable structured JSON responses
    - Verify call arguments (messages array, model, response_format)
    - Use `vi.fn()` to track calls and assert history accumulation
  </implementation>
</feature>

<verification>
```bash
cd server && npx vitest run src/provider/OpenAiProvider.test.ts
```
All tests pass. Structured output from mocked OpenAI responses produces valid `extractedPayload` that passes `CopilotStructuredOutputSchema.safeParse()`.
</verification>

<success_criteria>
- OpenAiProvider compiles and implements all three LlmProvider methods
- All unit tests pass with mocked OpenAI SDK
- startSession returns NormalizedMessage[] with extractedPayload
- sendMessage accumulates history across turns
- sendCardAction converts to text and delegates to sendMessage
- Structured output matches CopilotStructuredOutputSchema
- Model is configurable via constructor param (sourced from OPENAI_MODEL env var)
</success_criteria>

<output>
After completion, create `.planning/phases/26-openai-provider-implementation/26-01-SUMMARY.md`
</output>
